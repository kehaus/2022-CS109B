{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TooqzPJ_EyHI"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Introduction to Data Science\n",
    "\n",
    "## Lab 9 extras: Embeddings \n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2022**<br/>\n",
    "**Instructors**: Mark Glickman & Pavlos Protopapas<br/>\n",
    "**Authors**: Shivas Jayaram\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbeFIO6hFFaJ"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this Lab, you should understand how to:\n",
    "* Using **pretrained Embeddings** in a model\n",
    "* Review **Word2Vec** pretrained embeddings\n",
    "* Review **Glove** pretrained embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt8y_AYNHz0T"
   },
   "source": [
    "## **Setup Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_we-CjaH0Kr"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YSBEvu07Ejrs"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4wtdVNoIxSQ",
    "outputId": "a0ff8ded-9ae9-4587-8447-0ace9706de9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. \n",
      "Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, \n",
      "data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures \n",
      "such as CNNs, RNNs, transformers, language models, autoencoders, and generative models as well as basic \n",
      "Bayesian methods, and unsupervised learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. \n",
    "Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, \n",
    "data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures \n",
    "such as CNNs, RNNs, transformers, language models, autoencoders, and generative models as well as basic \n",
    "Bayesian methods, and unsupervised learning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Input Text:\",input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5aXHsd7m2v7"
   },
   "source": [
    "Tokenize the text using `tf.keras.preprocessing.text.Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAliduWimV7P",
    "outputId": "b664bdce-9fbf-4ac7-d883-eee679ec6456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('advanced', 2), ('topics', 2), ('in', 2), ('data', 6), ('science', 3), ('cs109b', 1), ('is', 1), ('the', 3), ('second', 1), ('half', 1), ('of', 1), ('a', 1), ('one', 1), ('year', 1), ('introduction', 2), ('to', 2), ('building', 1), ('upon', 1), ('material', 1), ('course', 1), ('introduces', 1), ('methods', 2), ('for', 1), ('wrangling', 1), ('visualization', 1), ('statistical', 1), ('modeling', 1), ('and', 3), ('prediction', 1), ('include', 1), ('big', 1), ('multiple', 1), ('deep', 1), ('learning', 2), ('architectures', 1), ('such', 1), ('as', 3), ('cnns', 1), ('rnns', 1), ('transformers', 1), ('language', 1), ('models', 2), ('autoencoders', 1), ('generative', 1), ('well', 1), ('basic', 1), ('bayesian', 1), ('unsupervised', 1)])\n",
      "word_index {'data': 1, 'science': 2, 'the': 3, 'and': 4, 'as': 5, 'advanced': 6, 'topics': 7, 'in': 8, 'introduction': 9, 'to': 10, 'methods': 11, 'learning': 12, 'models': 13, 'cs109b': 14, 'is': 15, 'second': 16, 'half': 17, 'of': 18, 'a': 19, 'one': 20, 'year': 21, 'building': 22, 'upon': 23, 'material': 24, 'course': 25, 'introduces': 26, 'for': 27, 'wrangling': 28, 'visualization': 29, 'statistical': 30, 'modeling': 31, 'prediction': 32, 'include': 33, 'big': 34, 'multiple': 35, 'deep': 36, 'architectures': 37, 'such': 38, 'cnns': 39, 'rnns': 40, 'transformers': 41, 'language': 42, 'autoencoders': 43, 'generative': 44, 'well': 45, 'basic': 46, 'bayesian': 47, 'unsupervised': 48}\n",
      "Vocabulary Size: 48\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit on text to generate token index and vocabulary\n",
    "tokenizer.fit_on_texts([input_text])\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.texts_to_sequences([input_text])\n",
    "\n",
    "print(tokenizer.word_counts)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "print(\"word_index\",word_index)\n",
    "vocabulary_size = len(word_index.keys())\n",
    "print(\"Vocabulary Size:\",vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atfggfyjT-Xh"
   },
   "source": [
    "## **Word2Vec Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS--oaEWUCH9"
   },
   "source": [
    "### **Download pretrained Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ivpzkm-AGGs4",
    "outputId": "8b83e9d2-3ed8-481c-f312-930c17e863ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_path: /home/u_61397728/.keras/datasets/GoogleNews-vectors-negative300.bin.gz\n",
      "Download execution time (mins) 0.00025364160537719724\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Dowload the news dataset\n",
    "word2vec_path = tf.keras.utils.get_file(\n",
    "    origin=\"https://github.com/dlops-io/datasets/releases/download/v1.0/GoogleNews-vectors-negative300.bin.gz\",\n",
    "    extract=False)\n",
    "print(\"word2vec_path:\",word2vec_path)\n",
    "execution_time = (time.time() - start_time)/60.0\n",
    "print(\"Download execution time (mins)\",execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuJCP76yUE25"
   },
   "source": [
    "### **Load pretrained Embedding**\n",
    "\n",
    "We need to prepare the pretrained embedding to use in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0vQH1UqGGve",
    "outputId": "9027c9b8-6580-4066-de0f-a686e0436c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load word2vec\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "print(\"Number of word vectors:\",word2vec.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cL8MFqjiU3b3",
    "outputId": "f4a9c985-3a50-42bc-8dff-b5a1a5d5e66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news : [-0.13867188  0.04370117 -0.13085938 -0.16796875 -0.06054688] , Shape: (300,)\n",
      "data : [-0.17285156 -0.14257812  0.04370117 -0.03344727 -0.07861328] , Shape: (300,)\n",
      "hurricane : [ 0.14453125 -0.11083984 -0.3671875   0.10449219  0.06396484] , Shape: (300,)\n",
      "political : [-0.02868652  0.02929688 -0.0625      0.35351562 -0.11181641] , Shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# View some word embeddings\n",
    "sample_embeddings_words = [\"news\", \"data\", \"hurricane\", \"political\"]\n",
    "for word in sample_embeddings_words:\n",
    "  print(word,\":\",word2vec[word][:5],\", Shape:\", word2vec[word].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7f7fc434c580>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZJN6xghVZBt",
    "outputId": "dc0d525c-c175-49fc-bba3-80a8e1e6abba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Coverage: 0.8333333333333334\n",
      "Embedding Matrix, Shape (49, 300)\n"
     ]
    }
   ],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_dim = 300\n",
    "\n",
    "# We want to select only the embeddings of our vocabulary to feed into our model in the next step\n",
    "embedding_matrix = np.zeros((vocabulary_size+1, embedding_dim))\n",
    "oov = {}    \n",
    "n_covered = 0\n",
    "n_oov = 0\n",
    "for word, i in word_index.items():\n",
    "  if word in word2vec:\n",
    "    embedding_matrix[i] = word2vec[word]\n",
    "  else:\n",
    "    n_oov += 1\n",
    "\n",
    "text_coverage = (vocabulary_size-n_oov)/vocabulary_size\n",
    "print(\"Text Coverage:\",text_coverage)\n",
    "\n",
    "print(\"Embedding Matrix, Shape\" ,embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YakhRNN9bjYs"
   },
   "source": [
    "So we can see 80% of our vocubulary has pretrained embeddings. The OOV will have values of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKibDn_DbvQt"
   },
   "source": [
    "### **Build Model using the pretrained Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGji3fRBbvQt",
    "outputId": "155c5aa1-5d23-45c1-db86-7b9f559e1d90"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model input\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_input \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Embedding Layer, with pre-trained weights\u001b[39;00m\n\u001b[1;32m      5\u001b[0m embedding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(input_dim\u001b[38;5;241m=\u001b[39membedding_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m      6\u001b[0m                             output_dim\u001b[38;5;241m=\u001b[39membedding_dim, \n\u001b[1;32m      7\u001b[0m                             embeddings_initializer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mConstant(embedding_matrix), \u001b[38;5;66;03m# Load pre-trained weights\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)(model_input)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Model input\n",
    "model_input = tf.keras.layers.Input(shape=(1))\n",
    "\n",
    "# Embedding Layer, with pre-trained weights\n",
    "embedding = tf.keras.layers.Embedding(input_dim=embedding_matrix.shape[0], \n",
    "                            output_dim=embedding_dim, \n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), # Load pre-trained weights\n",
    "                            name=\"embedding\")(model_input)\n",
    "# Create model\n",
    "model = tf.keras.Model(inputs=model_input, outputs=embedding)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2_syA1fbvQu",
    "outputId": "4212871c-1993-4aa8-c4ba-8d13f5926465"
   },
   "outputs": [],
   "source": [
    "word = \"data\"\n",
    "print(\"Embedding Value for word:\",word,model([word_index[word]])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEjZw7FJbvQu",
    "outputId": "989cac7b-07a8-4df6-8071-f1030d84c311"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding Value for word:\u001b[39m\u001b[38;5;124m\"\u001b[39m,word,\u001b[43mmodel\u001b[49m([word_index[word]])[:\u001b[38;5;241m50\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "word = \"learning\"\n",
    "print(\"Embedding Value for word:\",word,model([word_index[word]])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmgGDNs-bvQu",
    "outputId": "7f1b588a-7b49-4762-eaf2-5a4898b78dad"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcs109b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding Value for word:\u001b[39m\u001b[38;5;124m\"\u001b[39m,word,\u001b[43mmodel\u001b[49m([word_index[word]])[:\u001b[38;5;241m50\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "word = \"cs109b\"\n",
    "print(\"Embedding Value for word:\",word,model([word_index[word]])[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psr_jNZLH54h"
   },
   "source": [
    "## **GloVe Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arA-lXbLH-Z_"
   },
   "source": [
    "### **Download pretrained Embedding**\n",
    "\n",
    "* GloVe stands for Global Vectors, which is an open-source project developed by Stanford. It contains pre-trained word representations in various sizes, including 50-dimensional, 100-dimensional, 200-dimensional, and 300-dimensional.\n",
    "\n",
    "* We choose the 100d version.\n",
    "\n",
    "[Reference](http://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gidFuZqcGGx2",
    "outputId": "c8442326-f6f9-4a27-f1c9-d0fedece5a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_path: /root/.keras/datasets/glove.6B.100d.txt\n",
      "Download execution time (mins) 0.05553990205128988\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Dowload the news dataset\n",
    "glove_path = tf.keras.utils.get_file(\n",
    "    origin=\"https://github.com/shivasj/dataset-store/releases/download/v3.0/glove.6B.100d.txt.zip\",\n",
    "    extract=True)\n",
    "glove_path = glove_path.replace(\".zip\",\"\")\n",
    "print(\"glove_path:\",glove_path)\n",
    "execution_time = (time.time() - start_time)/60.0\n",
    "print(\"Download execution time (mins)\",execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF2k88syHHsy"
   },
   "source": [
    "### **Load pretrained Embedding**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWOKPaafHefi"
   },
   "source": [
    "We need to prepare the pretrained embedding to use in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDN46ncSGG0F",
    "outputId": "916c2acd-2d1b-489b-8dc4-4c7b1d288244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: 400000\n"
     ]
    }
   ],
   "source": [
    "# Build a dictionary with word and its vectors\n",
    "embeddings_index = {}\n",
    "with open(glove_path) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Number of word vectors:\",len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4AncxrLIcGI",
    "outputId": "6b6b4a47-6dd6-49e8-fd27-5a193ad5a4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : [-0.038194 -0.24487   0.72812  -0.39961   0.083172] , Shape: (100,)\n",
      ", : [-0.10767  0.11053  0.59812 -0.54361  0.67396] , Shape: (100,)\n",
      ". : [-0.33979  0.20941  0.46348 -0.64792 -0.38377] , Shape: (100,)\n",
      "of : [-0.1529  -0.24279  0.89837  0.16996  0.53516] , Shape: (100,)\n",
      "to : [-0.1897    0.050024  0.19084  -0.049184 -0.089737] , Shape: (100,)\n",
      "and : [-0.071953  0.23127   0.023731 -0.50638   0.33923 ] , Shape: (100,)\n",
      "in : [ 0.085703 -0.22201   0.16569   0.13373   0.38239 ] , Shape: (100,)\n",
      "a : [-0.27086   0.044006 -0.02026  -0.17395   0.6444  ] , Shape: (100,)\n",
      "\" : [-0.30457 -0.23645  0.17576 -0.72854 -0.28343] , Shape: (100,)\n",
      "'s : [ 0.58854 -0.2025   0.73479 -0.68338 -0.19675] , Shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# View some word embeddings\n",
    "sample_embeddings_words = list(embeddings_index.keys())[:10]\n",
    "for word in sample_embeddings_words:\n",
    "  print(word,\":\",embeddings_index[word][:5],\", Shape:\", embeddings_index[word].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "su25kkXZIfyc",
    "outputId": "d9af5463-346e-40ce-a682-efadb23ccc2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Coverage: 0.9166666666666666\n",
      "Embedding Matrix, Shape (49, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_dim = 100\n",
    "\n",
    "# We want to select only the embeddings of our vocabulary to feed into our model in the next step\n",
    "embedding_matrix = np.zeros((vocabulary_size+1, embedding_dim))\n",
    "oov = {}    \n",
    "n_covered = 0\n",
    "n_oov = 0\n",
    "for word, i in word_index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "  else:\n",
    "    n_oov += 1\n",
    "\n",
    "text_coverage = (vocabulary_size-n_oov)/vocabulary_size\n",
    "print(\"Text Coverage:\",text_coverage)\n",
    "\n",
    "print(\"Embedding Matrix, Shape\" ,embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R4RSxsPR8kL"
   },
   "source": [
    "So we can see 85% of our vocubulary has pretrained embeddings. The OOV will have values of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFtk1sNHTHZ2"
   },
   "source": [
    "### **Build Model using the pretrained Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JW69eQlySnXk",
    "outputId": "85a9bafc-d6ac-45f8-96b3-8953f818295b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 1, 100)            4900      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,900\n",
      "Trainable params: 4,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model input\n",
    "model_input = tf.keras.layers.Input(shape=(1))\n",
    "\n",
    "# Embedding Layer, with pre-trained weights\n",
    "embedding = tf.keras.layers.Embedding(input_dim=embedding_matrix.shape[0], \n",
    "                            output_dim=embedding_dim, \n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), # Load pre-trained weights\n",
    "                            name=\"embedding\")(model_input)\n",
    "# Create model\n",
    "model = tf.keras.Model(inputs=model_input, outputs=embedding)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMs4LJOzTRgt",
    "outputId": "79d4fc8a-77a1-4ef4-bc89-3d86ae4405cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Value for word: data tf.Tensor(\n",
      "[-0.47099    0.61577    0.68969   -0.18149    0.30778   -0.8415\n",
      " -0.41873   -0.20013    0.28184   -0.34005    0.77286   -0.22774\n",
      "  0.059854  -0.24141    0.87783    0.72043    0.64295    0.36245\n",
      "  0.41621    0.13001   -0.47074   -0.44664    0.47363    0.40755\n",
      " -1.0341    -1.1422     0.37436    0.24631   -0.67291    0.49177\n",
      "  0.46506    0.13608   -0.93796    0.51887    0.51549   -0.26506\n",
      " -0.14551    0.22517    0.35244   -0.79648   -0.42247   -0.90587\n",
      " -0.83998    0.45365   -0.72494   -0.12592    0.43661   -0.53661\n",
      "  0.020523  -0.74609    1.1925     0.15719    0.29318    0.92661\n",
      "  0.48236   -1.829     -0.012697  -0.37029    2.3618     0.33587\n",
      " -0.1544     0.14657   -0.11307   -0.02493    0.31933    0.28815\n",
      " -0.2963    -0.33032    1.4774     0.23739   -0.25313    0.61367\n",
      "  0.56811   -0.56991    0.48798    0.065367   0.28258   -0.13537\n",
      " -1.1096    -0.35971    0.85313    0.463     -1.1223     0.0071569\n",
      " -1.7636    -0.44547    1.2478    -0.37541   -0.21634    0.45937\n",
      " -0.11387    0.75576   -0.24423   -0.056482   0.54792   -0.30928\n",
      "  0.25919   -0.59612    0.27596    0.088012 ], shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "word = \"data\"\n",
    "print(\"Embedding Value for word:\",word,model([word_index[word]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvOsne9UTxUR",
    "outputId": "c50bc1e2-a2de-4b31-a399-4ce696d0ca38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Value for word: learning tf.Tensor(\n",
      "[ 0.64812    0.69878   -0.39947    0.77634   -0.13132    0.2024\n",
      " -0.33399   -0.0066588  0.061684   0.1885    -0.10559   -0.31316\n",
      " -0.082495  -0.080517   0.3858    -0.10302    0.049431   0.17216\n",
      " -0.59079    0.77068   -1.2768    -0.25187    0.2195    -0.20176\n",
      " -0.30581   -0.18518    0.010889  -0.07529   -0.34732    0.61998\n",
      " -0.99703    1.0516    -0.42071   -0.39635    0.32607   -0.40061\n",
      " -0.46462    0.69904    0.29567   -0.35309   -0.59074    0.28999\n",
      " -0.25732   -0.1317    -0.69798    0.49818    0.41503    0.1487\n",
      "  0.083347  -0.43543   -0.093969  -0.3543     0.014998   0.63593\n",
      "  0.54564   -1.8439     0.78842   -0.19836    1.5707     0.25988\n",
      "  0.20875    0.7521    -0.085488  -0.70717    0.094104   0.44485\n",
      "  0.087818  -0.34779    0.57148    0.18662   -0.29435    0.42928\n",
      "  0.28392   -0.61614   -0.34108    0.58192   -0.16388   -0.0081997\n",
      " -0.27162   -0.27112   -0.21471    0.37376   -0.5352    -0.060945\n",
      " -1.6317     0.85144    0.056035  -0.53861   -0.58383   -0.19612\n",
      " -0.33941   -0.3141     0.22999    0.32688    0.043012  -0.037482\n",
      " -0.092067  -1.0734     0.8924     0.41776  ], shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "word = \"learning\"\n",
    "print(\"Embedding Value for word:\",word,model([word_index[word]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hl7SRd9lT0y8",
    "outputId": "49ce899c-f99d-4745-9463-dda94b688fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Value for word: cs109b tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.], shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "word = \"cs109b\"\n",
    "print(\"Embedding Value for word:\",word,model([word_index[word]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmjefwEIpDeW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "cs109b_lab9_extras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python3 (default - all except pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
