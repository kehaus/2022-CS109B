{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmO29d4ux0Lq"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science \n",
    "## Homework 4: Convolutional Neural Networks\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2022**<br/>\n",
    "**Instructors**: Mark Glickman & Pavlos Protopapas\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "TPTLfywkx0Lw",
    "outputId": "c2ea408b-93c1-428d-ce53-05cf88cad644"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/\"\n",
    "    \"content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instructions\"></a>\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- This homework can be submitted **in pairs**.\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- Please **restart the kernel and run the entire notebook again before you submit.**\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code continues to work, restart the kernel and rerun your notebook periodically while working through this assignment. \n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. **Please use only the libraries provided in those imports.**\n",
    "\n",
    "- Please use `.head(...)` when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate and report $R^2$\", do not just output the value from a cell. Write a `print(...)` function that clearly labels the output, includes a reference to the calculated value, and rounds it to a reasonable number of digits. **Do not hard code values in your printed output**. For example, this is an appropriate print statement:\n",
    "```python\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- **Your plots MUST be clearly labeled and easy to read,** including clear labels for the $x$ and $y$ axes, a descriptive title (\"MSE plot\" is NOT a descriptive title; \"Training and validation MSE at varying degree polynomial regression models\" on the other hand is descriptive), a legend when appropriate, and clearly formatted text and graphics.\n",
    "\n",
    "- **Your code may also be evaluated for efficiency and clarity.** As a result, correct output is not always sufficient for full credit.\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J176st1ox57R",
    "outputId": "cd0ca1c6-804d-4418-cb69-62ab0cbed6fa"
   },
   "outputs": [],
   "source": [
    "# These 3 installs are needed ONLY when using Colab.\n",
    "# DO NOT RUN THESE ON JUPYTERHUB!!!\n",
    "\n",
    "# !pip install tensorflow_addons\n",
    "# !pip install tensorflow_datasets\n",
    "# !pip install tf_keras_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaPrtFjfx0L1",
    "outputId": "32fad8bb-f1f0-4f4a-9ec1-8aa53a6560b7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import imageio\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import requests\n",
    "import scipy.ndimage as ndimage\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout \n",
    "from tensorflow.keras.layers import Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soOeL1Tpx0L4"
   },
   "source": [
    "**Please run these cells below.** First we set our local working directory to ensure our provided code in PART 2 works correctly, and then we enable [TensorFlow eager execution](https://www.tensorflow.org/guide/eager) and print a summary of whether there are local GPUs available for training your models. Running this HW on JupyterHub is recommended. But as long as you are using the provided `cs109b.yml` conda environment, you should expect to see a TensorFlow version >=2.3.0, which should allow this notebook to run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v-UPOjnXx0L6"
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "working_dir = pathlib.Path().absolute()\n",
    "# Uncomment line below to debug if images don't show\n",
    "#print(working_dir)\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M08r1qFTx0L8",
    "outputId": "d408a9ef-fc1b-4def-d80a-5f4abf3cb634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.8.0\n",
      "keras version 2.8.0\n",
      "Eager Execution Enabled: True\n",
      "\n",
      "All Devices: \n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "\n",
      "Available GPUs: \n",
      "[]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 12:02:58.262577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-11 12:02:58.262850: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-11 12:02:58.262895: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kh-ThinkPad-T460s): /proc/driver/nvidia/version does not exist\n",
      "2022-03-11 12:02:58.266930: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment\n",
    "# that evaluates operations immediately, without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(f\"tensorflow version {tf.__version__}\")\n",
    "print(f\"keras version {tf.keras.__version__}\")\n",
    "print(f\"Eager Execution Enabled: {tf.executing_eagerly()}\\n\")\n",
    "\n",
    "devices = tf.config.get_visible_devices()\n",
    "print(f\"All Devices: \\n{devices}\\n\")\n",
    "print(f\"Available GPUs: \\n{tf.config.list_logical_devices('GPU')}\\n\")\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "tf.random.set_seed(2266)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7AgX8bWx0L-"
   },
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "- [**PART 1 [55 pts]: Building a Basic CNN Model**](#part1)\n",
    "  - [Overview](#part1intro)\n",
    "  - [Questions](#part1questions)\n",
    "  - [Solutions](#part1solutions)\n",
    "\n",
    "\n",
    "- [**PART 2 [45 pts]: Regression with CNN**](#part2)\n",
    "  - [Overview](#part2intro)\n",
    "  - [Questions](#part2questions)\n",
    "  - [Solutions](#part2solutions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzGonykqx0L_"
   },
   "source": [
    "## About this Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L555gy_1x0L_"
   },
   "source": [
    "In this homework, we will explore Convolutional Neural Networks (CNNs).\n",
    "\n",
    "- In [PART 1](#part1), we will begin by building a CNN to classify CIFAR-10 images, a standard pedagogical problem, and use saliency maps to understand where the network is placing its \"attention.\"\n",
    "\n",
    "\n",
    "- Then, in [PART 2](#part2), we will then see that CNNs are great for more than just classifying our images! They can serve as image input processing for a variety of tasks, as we will show by training a network on the CelebA dataset to rotate images of faces upright.\n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "\n",
    "- Convolutional neural networks are computationally intensive.\n",
    "- **We highly recommend that you train your model on a system using GPUs. For this, we recommend using the GPU-enabled JupyterHub environment provided to you as part of this course** (or you could also take a look at Google Colab's runtime settings for accessing a GPU-enabled environment free of cost).\n",
    "- Models that take hours to train on CPUs can be trained in just minutes when using GPUs.\n",
    "- Additionally, **if you become frustrated having to rerun your model every time you open your notebook, take a look at how to save your trained model weights for later use** (as is required in [PART 2, question 2.2.3](#q223)).\n",
    "\n",
    "**KERNEL CRASHES:**\n",
    "\n",
    "If your kernel crashes as you attempt to train your model, please check the following items:\n",
    "- Models with too many parameters might not fit in GPU memory. Try reducing the size of your model.\n",
    "- A large `batch_size` will attempt to load too many images in GPU memory. Avoid using a very large batch size.\n",
    "- Avoid creating multiple copies of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co7dmftEx0MB"
   },
   "source": [
    "<a id=\"part1\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 1 [55 pts]: Building a Basic CNN Model\n",
    "\n",
    "[Return to contents](#contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOHqvz5Ux0ME"
   },
   "source": [
    "<a id=\"part1intro\"></a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this question, you will use Keras to create a convolutional neural network for predicting the \"type of object\" shown in each image from the [CIFAR-10](https://keras.io/datasets/#cifar10-small-image-classification) dataset. This dataset contains 50,000 32x32 colored training images and 10,000 test images of the same size, with a total of 10 classes, representing the \"type of object\" shown in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX0OlDNcx0MF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"part1questions\"></a>\n",
    "\n",
    "### <div class='exercise'>PART 1: Questions</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"q11\"></a>\n",
    "\n",
    "**1.1** <span style='color:blue'>**Loading CIFAR-10 and Constructing the Model**</span>\n",
    "\n",
    "<a id=\"q111\"></a>\n",
    "\n",
    "**1.1.1** Load the CIFAR-10 dataset from the `tensorflow.keras.datasets.cifar10` import shown at the top of this notebook. Perform any preprocessing of the data that might be required for this dataset.\n",
    "\n",
    "<a id=\"q112\"></a>\n",
    "\n",
    "**1.1.2** Construct a classification model architecture using a combination of the following layers: Conv2D, MaxPooling2D, Dense, Dropout and Flatten. The layers don’t necessarily need to be in this order, and you can use as many of these types of layers as you’d like. \n",
    "\n",
    "  - You may use an existing architecture like AlexNet or VGG16, or create your own.\n",
    "\n",
    "  - However, you MUST code the network yourself and not use a pre-written implementation. \n",
    "\n",
    "  - You must have at least 2 Conv2D layers, and at least one of your Conv2D layers should have 9 or more filters in order to complete question 1.4.1.\n",
    "\n",
    "\n",
    "<a id=\"q12\"></a>\n",
    "\n",
    "**1.2** <span style='color:blue'>**Model parameter growth**</span>\n",
    "\n",
    "How does the number of total parameters change (e.g. linearly, exponentially, etc.) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture and increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship. **HINT:** Completing this question is far easier if you write a function that generates your desired architecture in 1.1.2, with arguments that allow you to easily rebuild the architecture with varying numbers of filters per layer.\n",
    "\n",
    "<a id=\"q13\"></a>\n",
    "\n",
    "**1.3** <span style='color:blue'>**Choose a model, train and evaluate it**</span>\n",
    "\n",
    "<a id=\"q131\"></a>\n",
    "\n",
    "**1.3.1** Print the model summary for your chosen architecture, and report the total number of parameters. Then train your model using the CIFAR-10 dataset, and `validation_split=0.2`. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs. Your validation and test accuracies should both exceed 70%.\n",
    "\n",
    "<a id=\"q132\"></a> \n",
    "\n",
    "**1.3.2** Plot the training loss and accuracy per epoch (both train and validation) for your chosen architecture.\n",
    " \n",
    "\n",
    "<a id=\"q14\"></a> \n",
    "\n",
    "**1.4** <span style='color:blue'>**Techniques to visualize the model**</span>\n",
    " \n",
    "We will gain an intuition into how our model is processing the inputs in two ways.  First we'll ask you to use activation maps to visualize the activations in the intermediate layers of the network. We've provided a helper function `get_activation_maps` to aid in extracting activation maps from layer outputs in your model network.  Feel free to take advantage of it if you'd like.  We'll also ask you to use [saliency maps](https://arxiv.org/abs/1312.6034) to visualize the pixels that have the largest impact on the classification of an input (image in this case), including a more recent development, a [Grad-CAM](https://arxiv.org/abs/1610.02391) saliency heatmap, which has been shown to better indicate the attention of CNNs.\n",
    " \n",
    "<a id=\"q141\"></a> \n",
    "\n",
    "**1.4.1** For a given input image from the test set that is correctly classified, use your model and extract 9 activation maps from an intermediate convolutional layer of your choice and plot the images in a 3x3 grid (use `imshow`'s `cmap='gray'` to show the activation maps in black & white).  Make sure to plot (and clearly label) your original input image as well. You may use the provided `get_activation_maps` function and the `cifar10dict` dictionary to convert class index to the correct class name.\n",
    "\n",
    "<a id=\"q142\"></a> \n",
    "\n",
    "**1.4.2** For the same input image generate and plot a SmoothGrad saliency map to show the pixels in the image most pertinent to classification, and a Grad-CAM saliency heatmap. This is most easily done with the [tf-keras-vis](https://pypi.org/project/tf-keras-vis/) package. Take a look at the \"Usage\" examples; it will be straightforward to apply to our model. Feel free to pick your own [colormap](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html); however, please stick to \"perceptionally uniform sequential\" or \"sequential\" colormaps because they are far easier to interpret (`jet` is neither). Arrange the three plots in a row using subplots: Original Image, SmoothGrad saliency Map, Grad-CAM saliency heatmap. Which visualization is easier to understand in your case, and what does the network seem to be focusing on?\n",
    "\n",
    "<a id=\"q143\"></a> \n",
    "\n",
    "**1.4.3** Repeat 1.4.2, but for an image from the test set that is **incorrectly classified**, indicating both the incorrect label and what the correct label should be, and from the visualizations of network attention, hypothesize why the network arrived at its answer. (Make sure you pass a new loss to the visualizers that uses the **incorrect** class index, because we want to see what caused the network to think the image was in that category!) If you had control over what images go in the training dataset, how could you modify it to avoid this particular network failure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1ybDR5zx0MI"
   },
   "source": [
    "<a id=\"part1solutions\"></a>\n",
    "\n",
    "## PART 1: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1**  <span style='color:blue'>**Loading CIFAR-10 and Constructing the Model**</span>\n",
    "\n",
    "<a id=\"q111\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1.1**  Load the CIFAR-10 dataset from the `tensorflow.keras.datasets.cifar10` import shown at the top of this notebook. Perform any preprocessing of the data that might be required for this dataset.\n",
    "\n",
    "<a id=\"q112\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21P6WuWVx0MJ",
    "outputId": "aa20b335-caf2-4fff-c849-62532498f931"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1.2**  Construct a classification model architecture using a combination of the following layers: Conv2D, MaxPooling2D, Dense, Dropout and Flatten. The layers don’t necessarily need to be in this order, and you can use as many of these types of layers as you’d like. \n",
    "\n",
    "  - You may use an existing architecture like AlexNet or VGG16, or create your own.\n",
    "\n",
    "  - However, you MUST code the network yourself and not use a pre-written implementation. \n",
    "\n",
    "  - You must have at least 2 Conv2D layers, and at least one of your Conv2D layers should have 9 or more filters in order to complete question 1.4.1.\n",
    "\n",
    "\n",
    "<a id=\"q12\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CvnFEcux0MK"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2**  <span style='color:blue'>**Model parameter growth**</span>\n",
    "\n",
    "How does the number of total parameters change (e.g. linearly, exponentially, etc.) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture and increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship. **HINT:** Completing this question is far easier if you write a function that generates your desired architecture in 1.1.2, with arguments that allow you to easily rebuild the architecture with varying numbers of filters per layer.\n",
    "\n",
    "<a id=\"q13\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnS8zEOmx0MM"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrzgLgOlx0MN"
   },
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDDe0DxUx0MN"
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3**  <span style='color:blue'>**Choose a model, train and evaluate it**</span>\n",
    "\n",
    "<a id=\"q131\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3.1**  Print the model summary for your chosen architecture, and report the total number of parameters. Then train your model using the CIFAR-10 dataset, and `validation_split=0.2`. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs. Your validation and test accuracies should both exceed 70%.\n",
    "\n",
    "<a id=\"q132\"></a> \n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHkmUJgix0MP"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3.2**  Plot the training loss and accuracy per epoch (both train and validation) for your chosen architecture.\n",
    " \n",
    "\n",
    "<a id=\"q14\"></a> \n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwXHX6i8x0MT"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4**  <span style='color:blue'>**Techniques to visualize the model**</span>\n",
    " \n",
    "We will gain an intuition into how our model is processing the inputs in two ways.  First we'll ask you to use activation maps to visualize the activations in the intermediate layers of the network. We've provided a helper function `get_activation_maps` to aid in extracting activation maps from layer outputs in your model network.  Feel free to take advantage of it if you'd like.  We'll also ask you to use [saliency maps](https://arxiv.org/abs/1312.6034) to visualize the pixels that have the largest impact on the classification of an input (image in this case), including a more recent development, a [Grad-CAM](https://arxiv.org/abs/1610.02391) saliency heatmap, which has been shown to better indicate the attention of CNNs.\n",
    " \n",
    "<a id=\"q141\"></a> \n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4.1**  For a given input image from the test set that is correctly classified, use your model and extract 9 activation maps from an intermediate convolutional layer of your choice and plot the images in a 3x3 grid (use `imshow`'s `cmap='gray'` to show the activation maps in black & white).  Make sure to plot (and clearly label) your original input image as well. You may use the provided `get_activation_maps` function and the `cifar10dict` dictionary to convert class index to the correct class name.\n",
    "\n",
    "<a id=\"q142\"></a> \n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LbMnQtCx0MG"
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "# A helper function to generate activation maps\n",
    "\n",
    "def get_activation_maps(model, layer_id, input_image):\n",
    "    \"\"\"Returns intermediate output (activation map) from passing\n",
    "    an image to the model\n",
    "    \n",
    "    Parameters:\n",
    "        model (tf.keras.Model): Model to examine\n",
    "        layer_id (int): Which layer's (from zero) output to return\n",
    "        input_image (ndarray): The input image\n",
    "    Returns:\n",
    "        maps (List[ndarray]): activation map stacked output by the\n",
    "        specified layer\n",
    "    \"\"\"\n",
    "    model_ = Model(\n",
    "        inputs=[model.input], outputs=[model.layers[layer_id].output]\n",
    "    )\n",
    "    return model_.predict(\n",
    "        np.expand_dims(input_image, axis=0)\n",
    "    )[0,:,:,:].transpose((2,0,1))\n",
    "\n",
    "\n",
    "# A dictionary to turn class index into class labels for CIFAR-10\n",
    "\n",
    "cifar10dict = {\n",
    "    0 : \"airplane\",\n",
    "    1 : \"automobile\",\n",
    "    2 : \"bird\",\n",
    "    3 : \"cat\",\n",
    "    4 : \"deer\",\n",
    "    5 : \"dog\",\n",
    "    6 : \"frog\",\n",
    "    7 : \"horse\",\n",
    "    8 : \"ship\",\n",
    "    9 : \"truck\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa00bKflx0MW"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4.2**  For the same input image generate and plot a SmoothGrad saliency map to show the pixels in the image most pertinent to classification, and a Grad-CAM saliency heatmap. This is most easily done with the [tf-keras-vis](https://pypi.org/project/tf-keras-vis/) package. Take a look at the \"Usage\" examples; it will be straightforward to apply to our model. Feel free to pick your own [colormap](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html); however, please stick to \"perceptionally uniform sequential\" or \"sequential\" colormaps because they are far easier to interpret (`jet` is neither). Arrange the three plots in a row using subplots: Original Image, SmoothGrad saliency Map, Grad-CAM saliency heatmap. Which visualization is easier to understand in your case, and what does the network seem to be focusing on?\n",
    "\n",
    "<a id=\"q143\"></a> \n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQqKzWUcx0Ma"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izEtpJqnx0Mb"
   },
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnE_CDq0x0Mb"
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4.3**  Repeat 1.4.2, but for an image from the test set that is **incorrectly classified**, indicating both the incorrect label and what the correct label should be, and from the visualizations of network attention, hypothesize why the network arrived at its answer. (Make sure you pass a new loss to the visualizers that uses the **incorrect** class index, because we want to see what caused the network to think the image was in that category!) If you had control over what images go in the training dataset, how could you modify it to avoid this particular network failure?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "prHq1ImTx0Mc",
    "outputId": "d184a8ec-b6c9-4d9f-f652-acc8d6a7ca54"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuDnxvWIx0Md"
   },
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK8N_mbbx0Md"
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRhtb4pMx0Md"
   },
   "source": [
    "<a id=\"part2\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 2 [45 pts]: Regression with CNN \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic1uIduQx0Me"
   },
   "source": [
    "<a id=\"part2intro\"></a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**In this problem we will construct a neural network to predict how far a face is from being \"upright\"**. \n",
    "\n",
    "**Image orientation estimation**\n",
    "\n",
    "Image orientation estimation with convolutional networks was first implemented in 2015 by Fischer, Dosovitskiy, and Brox in a paper titled [\"Image Orientation Estimation with Convolutional Networks\"](https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf). In that paper, the authors trained a network to straighten a wide variety of images using the [Microsoft COCO dataset](https://cocodataset.org/#home). \n",
    "\n",
    "**The modified CelebA dataset**\n",
    "\n",
    "In order to have a reasonable training time for a homework, we will be working on a subset of the problem where we just straighten images of faces. To do this:\n",
    "\n",
    "- We will be using the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset of celebrity faces, where we assume that professional photographers have taken level pictures;\n",
    "\n",
    "\n",
    "- The training will be supervised, with a rotated image (up to $\\pm 60^\\circ$) as an input, and the amount (in degrees) that the image has been rotated as a target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V3cWPARx0Mf",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"part2questions\"></a>\n",
    "\n",
    "### <div class='exercise'>PART 2: Questions</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "<a id=\"q21\"></a>\n",
    "\n",
    "**2.1** <span style='color:blue'>**Data preparation**</span>\n",
    "\n",
    "<a id=\"q211\"></a>\n",
    "\n",
    "**2.1.1** **Loading CelebA and Thinking about Datasets.** Run the cells provided to automatically download the CelebA dataset. It is about 1.3GB, which can take 10-15 minutes to download. This happens only once. In the future, when you rerun the cell, it will use the dataset already stored on your machine. Once downloaded, we load the CelebA image data as a TensorFlow Dataset object. [TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from PART 1 where the entire dataset was loaded in as an array. Datasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`).\n",
    "\n",
    "  - **Run the provided code**:\n",
    "\n",
    "    - The creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and test dataset `test_rot_ds`. \n",
    "  \n",
    "  - **Answer this question:**\n",
    "  \n",
    "    - Aside from pipelining, what is an important practical reason to use TensorFlow Dataset objects over simply loading all the data into $X$ and $Y$ `numpy` arrays? \n",
    "\n",
    "<a id=\"q212\"></a>\n",
    "\n",
    "**2.1.2** **Taking a look.** In a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. **HINT:** one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more information.\n",
    "\n",
    "<a id=\"q213\"></a>\n",
    "\n",
    "**2.1.3** **Conceptual Question.** Dropout layers have been shown to work well for regularizing deep neural networks, and can be used with very little computational cost. For our network, is it a good idea to use dropout layers? Explain, in **3-5 sentences**, being sure to explicitly discuss how a dropout layer works, and what that would mean for our model.\n",
    "\n",
    "<a id=\"q22\"></a>\n",
    "\n",
    "**2.2** <span style='color:blue'>**Building and training your CNN**</span>\n",
    "\n",
    "<a id=\"q221\"></a>\n",
    "\n",
    "**2.2.1** **Compiling your model.** Construct a model with multiple Conv layers and any other layers you think would help. Be certain to print your model summary as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we have been able to do it with substantially fewer. Any working setup is acceptable as long as you construct the network yourself and do not use a pre-written implementation.\n",
    "\n",
    "<a id=\"q222\"></a>\n",
    "\n",
    "**2.2.2** **Training your model.** Train your model. Please note that the `model.fit()` argument syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. However, your final model MUST be trained on all the available training data! You should achieve validation and test MSEs of less than 9, corresponding roughly to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want.\n",
    "\n",
    "<a id=\"q223\"></a>\n",
    "\n",
    "**2.2.3** **Saving and loading your weights.** Save your model weights to the path `model/your_model_name` where `your_model_name` is whatever filename prefix you want. Then reload your weights from that same path.\n",
    "  - **NOTE:** If you don't intend to use it, you may leave your line of code commented out. Nothing should change if you run it after saving it though, since it will load the same weights and everything else about the model will still be in memory. If you close your notebook or restart your kernel in the future, run all the cells required to compile the model, but skip the cells that performs the fit and the save. After running the load weights cell, your previously trained model will be restored.\n",
    "  - **Answer this question in a few sentences:** Suppose you save just the weights after training for a while. If you were to load the weights again and continue training, would it work? How will it be different than continuing from a full-model save?\n",
    "\n",
    "<a id=\"q224\"></a>\n",
    "\n",
    "**2.2.4** **Evaluating your model.** Create a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like the image shown below. This can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network.\n",
    "\n",
    "![straightened face](data/straightened.png)\n",
    "\n",
    "\n",
    "<a id=\"q23\"></a>\n",
    "\n",
    "**2.3** <span style='color:blue'>**Further Analysis**</span>\n",
    "\n",
    "<a id=\"q231\"></a>\n",
    "\n",
    "**2.3.1** **Correct an image of your choosing.** Find an image or image(s) (not from the provided test/training sets), or make your own. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.] (you may use the provided `normalize_image` function) and use your network to correct it.\n",
    "\n",
    "![Confused Chris](data/chrisprattcorrection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLUnwCLHx0Mf"
   },
   "source": [
    "<a id=\"part2solutions\"></a>\n",
    "\n",
    "## PART 2: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1**  <span style='color:blue'>**Data preparation**</span>\n",
    "\n",
    "<a id=\"q211\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1.1**  **Loading CelebA and Thinking about Datasets.** Run the cells provided to automatically download the CelebA dataset. It is about 2.5GB, which can take 10-20 minutes to download. This happens only once. In the future, when you rerun the cell, it will use the dataset already stored on your machine. Once downloaded, we load the CelebA image data as a TensorFlow Dataset object. [TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from PART 1 where the entire dataset was loaded in as an array. Datasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`).\n",
    "\n",
    "  - **Run the provided code**:\n",
    "\n",
    "    - The creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and test dataset `test_rot_ds`. \n",
    "  \n",
    "  - **Answer this question:**\n",
    "  \n",
    "    - Aside from pipelining, what is an important practical reason to use TensorFlow Dataset objects over simply loading all the data into $X$ and $Y$ `numpy` arrays? \n",
    "\n",
    "<a id=\"q212\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dvdi1Kpx0Mi"
   },
   "outputs": [],
   "source": [
    "# Run this cell to define our download_celeb function\n",
    "\n",
    "def download_celeb(\n",
    "    url, \n",
    "    filename,\n",
    "    filepath,\n",
    "    dirname,\n",
    "    dirpath,\n",
    "    chunk_size=1204,\n",
    "    overwrite=False,\n",
    "):\n",
    "    \"\"\"Downloads and extracts CelebA dataset from CS109B S3 bucket\"\"\"\n",
    "    \n",
    "    # Do not download if data already exists and overwrite==False\n",
    "    if not overwrite and os.path.isdir(os.path.join(dirpath, \"2.0.1\")):\n",
    "        print(\n",
    "            \"Congratulations...the CelebA dataset already exists \"\n",
    "            \"locally!\\nNo new downloads are required :o)\\n\"\n",
    "        )\n",
    "    # Download and extract CelebA if it doesn't already exist\n",
    "    else:\n",
    "        print(\"Downloading CelebA dataset to {}\\n\".format(filepath))\n",
    "\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            chunk_size = 1024\n",
    "            length = int(r.headers['content-length'])\n",
    "            print(\n",
    "                \"...downloading a {:.2f} GB file.\"\n",
    "                \"This is going to take a while!\".format(length/1e9)\n",
    "            )\n",
    "            time.sleep(0.5)\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in tqdm.tqdm(\n",
    "                    r.iter_content(chunk_size=chunk_size),\n",
    "                    total=int(length/chunk_size),\n",
    "                    unit=\"KB\"\n",
    "                ):\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(\"...{} download complete :o)\".format(filename))\n",
    "\n",
    "        if not os.path.isdir(dirpath):\n",
    "            os.makedirs(dirpath)\n",
    "\n",
    "        print(\n",
    "            \"...extracting {}. This will take a while too :o(\\n\"\n",
    "            \"\".format(filename)\n",
    "        )\n",
    "\n",
    "        with zipfile.ZipFile(filepath, 'r') as zipobj:\n",
    "            zipobj.extractall(dirpath)\n",
    "\n",
    "        print(\n",
    "            \"The CelebA dataset has been extracted to:\"\n",
    "            \"\\n\\n\\t{}\\n\".format(dirpath)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zevnEGhYx0Mj",
    "outputId": "2eb2ebab-0186-4830-edf2-ac6824129b27"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Download the CelebA dataset from the CS109B S3 bucket\n",
    "url = \"https://cs109b-prod-course-data.s3.amazonaws.com/CelebA/2.0.1.zip\"\n",
    "filename = \"2.0.1.zip\"\n",
    "dirname = \"data/celeb_a\"\n",
    "dirpath = os.path.join(working_dir, dirname)\n",
    "filepath = os.path.join(working_dir, filename)\n",
    "\n",
    "# Running on JupyterHub with data\n",
    "if os.path.isdir('/home/course_data/celeb_a/2.0.1/'):\n",
    "    data_dir = '/home/course_data'\n",
    "# Running anywhere else\n",
    "else:\n",
    "    data_dir = os.path.join(working_dir, \"data\")\n",
    "    download_celeb(url, filename, filepath, dirname, dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6grMeNex0Mk"
   },
   "outputs": [],
   "source": [
    "# This command will use the celeb_a dataset that you downloaded,\n",
    "# and load it into train and test \"tensorflow.Datasets\"\n",
    "\n",
    "train_celeb, test_celeb = tfds.load(\n",
    "    \"celeb_a\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=False,\n",
    "    data_dir = data_dir,\n",
    "    download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vrAkTQSx0Ml"
   },
   "outputs": [],
   "source": [
    "# You may use the following two functions\n",
    "def normalize_image(img):\n",
    "    return tf.cast(img, tf.float32)/255.\n",
    "\n",
    "def rot_resize(img, deg):\n",
    "    rotimg = ndimage.rotate(img, deg, reshape=False, order=3)\n",
    "    rotimg = np.clip(rotimg, 0., 1.)\n",
    "    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n",
    "    return rotimg\n",
    "\n",
    "################################################################\n",
    "# Don't manually invoke these functions; they are for Dataset \n",
    "# pipelining that is already done for you.\n",
    "################################################################\n",
    "def tf_rot_resize(img, deg):\n",
    "    \"\"\"Dataset pipe that rotates an image and resizes it to 140x120\"\"\"\n",
    "    rotimg = tfa.image.rotate(img, deg/180.*np.pi, interpolation=\"BILINEAR\")\n",
    "    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n",
    "    return rotimg\n",
    "\n",
    "def tf_random_rotate_helper(image):\n",
    "    \"\"\"Dataset pipe that normalizes image to [0.,1.] and rotates by a random\n",
    "    amount of degrees in [-60.,60.], returning an (input,target) pair consisting\n",
    "    of the rotated and resized image and the degrees it has been rotated by.\"\"\"\n",
    "    image = normalize_image(image)\n",
    "    deg = tf.random.uniform([],-60.,60.)\n",
    "    return (tf_rot_resize(image,deg), deg)  # (data, label)\n",
    "\n",
    "def tf_random_rotate_image(element):\n",
    "    \"\"\"Given an element drawn from the CelebA dataset, this returns a rotated\n",
    "    image and the amount it has been rotated by, in degrees.\"\"\"\n",
    "    image = element['image']\n",
    "    image, label = tf_random_rotate_helper(image)\n",
    "    image.set_shape((140,120,3))\n",
    "    return image, label\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYXdRVY8x0Mm"
   },
   "outputs": [],
   "source": [
    "# Pipeline for creating randomly rotated images with their target labels being \n",
    "# the amount they were rotated, in degrees.\n",
    "train_rot_ds = train_celeb.map(tf_random_rotate_image)\n",
    "test_rot_ds = test_celeb.map(tf_random_rotate_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKhUF5rRx0Mm"
   },
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1utW3oPax0Mn"
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1.2**  **Taking a look.** In a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. **HINT:** one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more information.\n",
    "\n",
    "<a id=\"q213\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "VHy76uiox0Mo",
    "outputId": "243ef68a-9a00-4b25-bbc8-11cb81730118"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1.3**  **Conceptual Question.** Dropout layers have been shown to work well for regularizing deep neural networks, and can be used with very little computational cost. For our network, is it a good idea to use dropout layers? Explain, in **3-5 sentences**, being sure to explicitly discuss how a dropout layer works, and what that would mean for our model.\n",
    "\n",
    "<a id=\"q22\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPB0-yfjx0Mq"
   },
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJsdhEUVx0Mr"
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2**  <span style='color:blue'>**Building and training your CNN**</span>\n",
    "\n",
    "<a id=\"q221\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2.1**  **Compiling your model.** Construct a model with multiple Conv layers and any other layers you think would help. Be certain to print your model summary as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we have been able to do it with substantially fewer. Any working setup is acceptable as long as you construct the network yourself and do not use a pre-written implementation.\n",
    "\n",
    "<a id=\"q222\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaD3HlKNx0Mr",
    "outputId": "17a0e1d3-29c7-43fb-adc2-a1dc365ed52f"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2.2**  **Training your model.** Train your model. Please note that the `model.fit()` argument syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. However, your final model MUST be trained on all the available training data! You should achieve validation and test MSEs of less than 9, corresponding roughly to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want.\n",
    "\n",
    "<a id=\"q223\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_shHXiHx0Mt"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2.3**  **Saving and loading your weights.** Save your model weights to the path `model/your_model_name` where `your_model_name` is whatever filename prefix you want. Then reload your weights from that same path.\n",
    "  - **NOTE:** If you don't intend to use it, you may leave your line of code commented out. Nothing should change if you run it after saving it though, since it will load the same weights and everything else about the model will still be in memory. If you close your notebook or restart your kernel in the future, run all the cells required to compile the model, but skip the cells that performs the fit and the save. After running the load weights cell, your previously trained model will be restored.\n",
    "  - **Answer this question in a few sentences:** Suppose you save just the weights after training for a while. If you were to load the weights again and continue training, would it work? How will it be different than continuing from a full-model save?\n",
    "\n",
    "<a id=\"q224\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFVM_NMbx0Mv",
    "outputId": "552aef1d-9a85-4dfa-e479-5611ffcc24fd"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiUsAwc-x0Mw"
   },
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qw6-MPrx0Mx"
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2.4**  **Evaluating your model.** Create a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like the image shown below. This can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network.\n",
    "\n",
    "![straightened face](data/straightened.png)\n",
    "\n",
    "\n",
    "<a id=\"q23\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWsHliY1x0My"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3**  <span style='color:blue'>**Further Analysis**</span>\n",
    "\n",
    "<a id=\"q231\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3.1**  **Correct an image of your choosing.** Find an image or image(s) (not from the provided test/training sets), or make your own. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.] (you may use the provided `normalize_image` function) and use your network to correct it.\n",
    "\n",
    "![Confused Chris](data/chrisprattcorrection.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln75ejPkx0M1"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cs109b-hw4-solutions-colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py-cs109b",
   "language": "python",
   "name": "py-cs109b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
